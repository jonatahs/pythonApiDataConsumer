doi,title,rank,pdf_url,abstract,publisher
10.1109/BigData.Congress.2013.13,A Discussion of Privacy Challenges in User Profiling with Big Data Techniques: The EEXCESS Use Case,1,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597115,"User profiling is the process of collecting information about a user in order to construct their profile. The information in a user profile may include various attributes of a user such as geographical location, academic and professional background, membership in groups, interests, preferences, opinions, etc. Big data techniques enable collecting accurate and rich information for user profiles, in particular due to their ability to process unstructured as well as structured information in high volumes from multiple sources. Accurate and rich user profiles are important for applications such as recommender systems, which try to predict elements that a user has not yet considered but may find useful. The information contained in user profiles is personal and thus there are privacy issues related to user profiling. In this position paper, we discuss user profiling with big data techniques and the associated privacy challenges. We also discuss the ongoing EU-funded EEXCESS project as a concrete example of constructing user profiles with big data techniques and the approaches being considered for preserving user privacy.",IEEE
10.1109/BigData.Congress.2013.15,Engineering Privacy for Big Data Apps with the Unified Modeling Language,2,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597117,"This paper describes proposed privacy extensions to UML to help software engineers to quickly visualize privacy requirements, and design privacy into big data applications. To adhere to legal requirements and/or best practices, big data applications will need to apply Privacy by Design principles and use privacy services, such as, and not limited to, anonymization, pseudonymization, security, notice on usage, and consent for usage. We extend UML with ribbon icons representing needed big data privacy services. We further illustrate how privacy services can be usefully embedded in use case diagrams using containers. These extensions to UML help software engineers to visually and quickly model privacy requirements in the analysis phase, this phase is the longest in any software development effort. As proof of concept, a prototype based on our privacy extensions to Microsoft Visio's UML is created and the utility of our UML privacy extensions to the Use Case Diagram artifact is illustrated employing an IBM Watson-like commercial use case on big data in a health sector application.",IEEE
10.1109/BigData.Congress.2013.16,Milieu: Lightweight and Configurable Big Data Provenance for Science,3,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597118,"The volume and complexity of data produced and analyzed in scientific collaborations is growing exponentially. It is important to track scientific data-intensive analysis workflows to provide context and reproducibility as data is transformed in these collaborations. Provenance addresses this need and aids scientists by providing the lineage or history of how data is generated, used and modified. Provenance has traditionally been collected at the workflow level often making it hard to capture relevant information about resource characteristics and is difficult for users to easily incorporate in existing workflows. In this paper, we describe Milieu, a framework focused on the collection of provenance for scientific experiments in High Performance Computing systems. Our approach collects provenance in a minimally intrusive way without significantly impacting the performance of the execution of scientific workflows. We also provide fidelity to our provenance collection by allowing users to specify three levels of provenance collection. We evaluate our framework on systems at the National Energy Research Scientific Computing Center (NERSC) and show that the overhead is less than the variation already experienced by these applications in these shared environments.",IEEE
10.1109/BigData.Congress.2013.17,Consistent Process Mining over Big Data Triple Stores,4,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597119,"'Big Data' techniques are often adopted in cross-organization scenarios for integrating multiple data sources to extract statistics or other latent information. Even if these techniques do not require the support of a schema for processing data, a common conceptual model is typically defined to address name resolution. This implies that each local source is tasked of applying a semantic lifting procedure for expressing the local data in term of the common model. Semantic heterogeneity is then potentially introduced in data. In this paper we illustrate a methodology designed to the implementation of consistent process mining algorithms in a `Big Data' context. In particular, we exploit two different procedures. The first one is aimed at computing the mismatch among the data sources to be integrated. The second uses mismatch values to extend data to be processed with a traditional map reduce algorithm.",IEEE
10.1109/BigData.Congress.2013.18,Towards Cloud-Based Analytics-as-a-Service (CLAaaS) for Big Data Analytics in the Cloud,5,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597120,"Data Analytics has proven its importance in knowledge discovery and decision support in different data and application domains. Big data analytics poses a serious challenge in terms of the necessary hardware and software resources. The cloud technology today offers a promising solution to this challenge by enabling ubiquitous and scalable provisioning of the computing resources. However, there are further challenges that remain to be addressed such as the availability of the required analytic software for various application domains, estimation and subscription of necessary resources for the analytic job or workflow, management of data in the cloud, and design, verification and execution of analytic workflows. We present a taxonomy for analytic workflow systems to highlight the important features in existing systems. Based on the taxonomy and a study of the existing analytic software and systems, we propose the conceptual architecture of CLoud-based Analytics-as-a-Service (CLAaaS), a big data analytics service provisioning platform, in the cloud. We outline the features that are important for CLAaaS as a service provisioning system such as user and domain specific customization and assistance, collaboration, modular architecture for scalable deployment and Service Level Agreement.",IEEE
10.1109/BigData.Congress.2013.21,Towards a Quality-centric Big Data Architecture for Federated Sensor Services,6,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597123,"As the Internet of Things (IoT) paradigm gains popularity, the next few years will likely witness 'servitization' of domain sensing functionalities. We envision a cloud-based eco-system in which high quality data from large numbers of independently-managed sensors is shared or even traded in real-time. Such an eco-system will necessarily have multiple stakeholders such as sensor data providers, domain applications that utilize sensor data (data consumers), and cloud infrastructure providers who may collaborate as well as compete. While there has been considerable research on wireless sensor networks, the challenges involved in building cloud-based platforms for hosting sensor services are largely unexplored. In this paper, we present our vision for data quality (DQ)-centric big data infrastructure for federated sensor service clouds. We first motivate our work by providing real-world examples. We outline the key features that federated sensor service clouds need to possess. This paper proposes a big data architecture in which DQ is pervasive throughout the platform. Our architecture includes a markup language called SDQ-ML for describing sensor services as well as for domain applications to express their sensor feed requirements. The paper explores the advantages and limitations of current big data technologies in building various components of the platform. We also outline our initial ideas towards addressing the limitations.",IEEE
10.1109/BigData.Congress.2013.23,Multi-resolution Social Network Community Identification and Maintenance on Big Data Platform,7,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597125,"Community identification in social networks is of great interest and with dynamic changes to its graph representation and content, the incremental maintenance of community poses significant challenges in computation. Moreover, the intensity of community engagement can be distinguished at multiple levels, resulting in a multi-resolution community representation that has to be maintained over time. In this paper, we first formalize this problem using the k-core metric projected at multiple k values, so that multiple community resolutions are represented with multiple k-core graphs. We then present distributed algorithms to construct and maintain a multi-k-core graph, implemented on the scalable big-data platform Apache HBase. Our experimental evaluation results demonstrate orders of magnitude speedup by maintaining multi-k-core incrementally over complete reconstruction. Our algorithms thus enable practitioners to create and maintain communities at multiple resolutions on different topics in rich social network content simultaneously.",IEEE
10.1109/BigData.Congress.2013.24,Approximate Incremental Big-Data Harmonization,8,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597127,"The needs of `big data analytics' increasingly require IT organizations to ingest, process, and extract business insights from ever larger volumes of data that arrive far more rapidly than before, as well as from new sources such as social media, mobile devices, and sensors. However, in order to extract insights from diverse information feeds from multiple, often unrelated sources, these first need to be correlated or harmonized to a common level of granularity. We formally define this commonly arising data harmonization problem. We show how to correlate disparate data sources using map-reduce, but in an approximate and/or incremental manner as often required in practice. We motivate our techniques through a real-life enterprise data-harmonization case study for which we describe our performance results on big-data technologies, namely, Map Reduce, Hadoop and PIG.",IEEE
10.1109/BigData.Congress.2013.25,Countering the Concept-Drift Problem in Big Data Using iOVFDT,9,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597128,"How to efficiently uncover the knowledge hidden within massive and big data remains an open problem. One of the challenges is the issue of 'concept drift' in streaming data flows. Concept drift is a well-known problem in data analytics, in which the statistical properties of the attributes and their target classes shift over time, making the trained model less accurate. Many methods have been proposed for data mining in batch mode. Stream mining represents a new generation of data mining techniques, in which the model is updated in one pass whenever new data arrive. This one-pass mechanism is inherently adaptive and hence potentially more robust than its predecessors in handling concept drift in data streams. In this paper, we evaluate the performance of a family of decision-tree-based data stream mining algorithms. The advantage of incremental decision tree learning is the set of rules that can be extracted from the induced model. The extracted rules, in the form of predicate logics, can be used subsequently in many decision-support applications. However, the induced decision tree must be both accurate and compact, even in the presence of concept drift. We compare the performance of three typical incremental decision tree algorithms (VFDT [2], ADWIN [3], iOVFDT [4]) in dealing with concept-drift data. Both synthetic and real-world drift data are used in the experiment. iOVFDT is found to produce superior results.",IEEE
10.1109/BigData.Congress.2013.39,Challenges of Privacy Protection in Big Data Analytics,10,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597142,"The big data paradigm implies that almost every type of information eventually can be derived from sufficiently large datasets. However, in such terms, linkage of personal data of individuals poses a severe threat to privacy and civil rights. In this position paper, we propose a set of challenges that have to be addressed in order to perform big data analytics in a privacy-compliant way.",IEEE
10.1109/BigData.Congress.2013.78,Techniques for Graph Analytics on Big Data,11,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597145,"Graphs enjoy profound importance because of their versatility and expressivity. They can be effectively used to represent social networks, web search engines and genome sequencing. The field of graph pattern matching has been of significant importance and has wide-spread applications. Conceptually, we want to find subgraphs that match a pattern in a given graph. Much work has been done in this field with solutions like Subgraph Isomorphism and Regular Expression matching. With Big Data, scientists are frequently running into massive graphs that have amplified the challenge that this area poses. We study the speedup and communication behavior of three distributed algorithms for inexact graph pattern matching. We also study the impact of different graph partitionings on runtime and network I/O. Our extensive results show that the algorithms exhibit excellent scalable behavior and min-cut partitioning can lead to improved performance under some circumstances, and can drastically reduce the network traffic as well.",IEEE
10.1109/BigData.Congress.2013.60,Service-Generated Big Data and Big Data-as-a-Service: An Overview,12,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597164,"With the prevalence of service computing and cloud computing, more and more services are emerging on the Internet, generating huge volume of data, such as trace logs, QoS information, service relationship, etc. The overwhelming service-generated data become too large and complex to be effectively processed by traditional approaches. How to store, manage, and create values from the service-oriented big data become an important research problem. On the other hand, with the increasingly large amount of data, a single infrastructure which provides common functionality for managing and analyzing different types of service-generated big data is urgently required. To address this challenge, this paper provides an overview of service-generated big data and Big Data-as-a-Service. First, three types of service-generated big data are exploited to enhance system performance. Then, Big Data-as-a-Service, including Big Data Infrastructure-as-a-Service, Big Data Platform-as-a-Service, and Big Data Analytics Software-as-a-Service, is employed to provide common big data related services (e.g., accessing service-generated big data and data analytics results) to users to enhance efficiency and reduce cost.",IEEE
10.1109/BigData.Congress.2013.61,Big Data Infrastructure for Active Situation Awareness on Social Network Services,13,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597165,"Awareness computing aims at our final goal in computer science to simulate human's awareness and cognition. Awareness of social network knowledge in everyday life is actively enabled by big data society. In this paper, we investigate infrastructure for big data analytics for social network services, and propose TF-IDF calculation on big data infrastructure to be aware of social relations on social networks.",IEEE
10.1109/BigData.Congress.2013.65,Analysis of Technology Trends Based on Big Data,14,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597169,"The paper suggests a method for analyzing technology trends. The process, which investigates development of technologies over time, identifies main technologies displaying the fastest growth compared to greater influence of new inventions. The method analyzes term frequency and change over time of technological terms in patents to identify the prior technologies that lead to a new technology and detects technologies that have the biggest impact. The analysis was performed on 4,354,054 patents from the US Patent Office dating from 1975 until today. Some correlation is displayed between technology trends and future US stock market performance.",IEEE
10.1109/BigData.Congress.2013.66,Storage Mining: Where IT Management Meets Big Data Analytics,15,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597170,"The emerging paradigm shift to cloud based data center infrastructures imposes remarkable challenges to IT management operations, e.g., due to virtualization techniques and more stringent requirements for cost and efficiency. On one hand, the voluminous data generated by daily IT operations such as logs and performance measurements contain abundant information and insights which can be leveraged to assist the IT management. On the other hand, traditional IT management solutions cannot consume and exploit the rich information contained in the data due to the daunting volume, velocity, variety, as well as the lack of scalable data mining and machine learning frameworks to extract insights from such raw data. In this paper, we present our on-going research thrust of designing novel IT management solutions by leveraging big data analytics frameworks. As an example, we introduce our project of Storage Mining, which exploits big data analytics techniques to facilitate storage cloud management. The challenges are discussed and our proof-of-concept big data analytics framework is presented.",IEEE
10.1109/BigData.Congress.2013.68,Distributed Stochastic Aware Random Forests -- Efficient Data Mining for Big Data,16,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597172,"Some top data mining algorithms, as ensemble classifiers, may be inefficient to very large data set. This paper makes an initial proposal of a distributed ensemble classifier algorithm based on the popular Random Forests for Big Data. The proposed algorithm aims to improve the efficiency of the algorithm by a distributed processing model called MapReduce. At the same time, our proposed algorithm aims to reduce the randomness impact by following an algorithm called Stochastic Aware Random Forests - SARF.",IEEE
10.1109/BigData.Congress.2013.70,The Knowledge Service Project in the Era of Big Data,17,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597174,"The integration of industrialization and IT application is going to be one of the Chinese new economy development strategies in the future. Therefore, how to make Big Data useful in generating significant productivity improvement in industries has already become one of the most important issues. This paper outlines the platform of knowledge service based on big data processing techniques, which have guided the implementation of the integration of industrialization and IT application in Shenyang. And some challenges we met during implementation of the project were also discussed.",IEEE
10.1109/BigData.Congress.2013.79,Effective Interpretation of Bucket Testing Results through Big Data Analytics,18,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6597179,"Bucket testing is a common practice in the internet industry, where new features and services are tested by exposing them to a randomly selected small subset of users. However, in this simple version of bucket testing, since a very small fraction of the total users are selected through uniform independent sampling of the population, the samples chosen, at times, do not adequately serve as a reasonable statistical proxy for the total population. This may lead to erroneous interpretation of the bucket testing results, particularly for online sites having large audiences with varying demographics and preferences. In this work, we present a novel algorithmic framework that addresses this challenge and provides an efficient and more accurate interpretation of the bucket testing results by analyzing the big audience data and factoring in the nature of the overall population in terms of the different user attributes. We demonstrate the effectiveness of our algorithm through the data obtained from real experiments conducted on Yahoo's bucket testing platform.",IEEE
10.1109/BigData.2013.6691547,Security — A big question for big data,19,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691547,"Summary form only given. Big data implies performing computation and database operations for massive amounts of data, remotely from the data owner's enterprise. Since a key value proposition of big data is access to data from multiple and diverse domains, security and privacy will play a very important role in big data research and technology. The limitations of standard IT security practices are well-known, making the ability of attackers to use software subversion to insert malicious software into applications and operating systems a serious and growing threat whose adverse impact is intensified by big data. So, a big question is what security and privacy technology is adequate for controlled assured sharing for efficient direct access to big data. Making effective use of big data requires access from any domain to data in that domain, or any other domain it is authorized to access. Several decades of trusted systems developments have produced a rich set of proven concepts for verifiable protection to substantially cope with determined adversaries, but this technology has largely been marginalized as “overkill” and vendors do not widely offer it. This talk will discuss pivotal choices for big data to leverage this mature security and privacy technology, while identifying remaining research challenges.",IEEE
10.1109/BigData.2013.6691549,Communication efficient algorithms for fundamental big data problems,20,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691549,"Big Data applications often store or obtain their data distributed over many computers connected by a network. Since the network is usually slower than the local memory of the machines, it is crucial to process the data in such a way that not too much communication takes place. Indeed, only communication volume sublinear in the input size may be affordable. We believe that this direction of research deserves more intensive study. We give examples for several fundamental algorithmic problems where nontrivial algorithms with sublinear communication volume are possible. Our main technical contribution are several related results on distributed Bloom filter replacements, duplicate detection, and data base join. As an example of a very different family of techniques, we discuss linear programming in low dimensions.",IEEE
10.1109/BigData.2013.6691551,P-DOT: A model of computation for big data,21,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691551,"In response to the high demand of big data analytics, several programming models on large and distributed cluster systems have been proposed and implemented, such as MapRe-duce, Dryad and Pregel. However, compared with high performance computing areas, the basis and principles of computation and communication behavior of big data analytics is not well studied. In this paper, we review the current big data computational model DOT and DOTA, and propose a more general and practical model p-DOT (p-phases DOT). p-DOT is not a simple extension, but with profound significance: for general aspects, any big data analytics job execution expressed in DOT model or BSP model can be represented by it; for practical aspects, it considers I/O behavior to evaluate performance overhead. Moreover, we provide a cost function implying that the optimal number of machines is near-linear to the square root of input size for a fixed algorithm and workload, and demonstrate the effectiveness of the function through several experiments.",IEEE
10.1109/BigData.2013.6691553,Elastic algorithms for guaranteeing quality monotonicity in big data mining,22,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691553,"When mining large data volumes in big data applications users are typically willing to use algorithms that produce acceptable approximate results satisfying the given resource and time constraints. Two key challenges arise when designing such algorithms. The first relates to reasoning about tradeoffs between the quality of data mining output, e.g. prediction accuracy for classification tasks and available resource and time budgets. The second is organizing the computation of the algorithm to guarantee producing better quality of results as more budget is used. Little work has addressed these two challenges together in a generic way. In this paper, we propose a novel framework for developing elastic big data mining algorithms. Based on Shannon's entropy, an information-theoretic approach is introduced to reason about how result quality is affected by the allocated budget. This is then used to guide the development of algorithms that adapt to the available time budgets while guaranteeing producing better quality results as more budgets are used. We demonstrate the application of the framework by developing elastic k-Nearest Neighbour (kNN) classification and collaborative filtering (CF) recommendation algorithms as two examples. The core of both elastic algorithms is to use a naïve kNN classification or CF algorithm over R-tree data structures that successively approximate the entire datasets. Experimental evaluation was performed using prediction accuracy as quality metric on real datasets. The results show that elastic mining algorithms indeed produce results with consistent increase in observable qualities, i.e., prediction accuracy, in practice.",IEEE
10.1109/BigData.2013.6691556,Storing and manipulating environmental big data with JASMIN,23,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691556,"JASMIN is a super-data-cluster designed to provide a high-performance high-volume data analysis environment for the UK environmental science community. Thus far JASMIN has been used primarily by the atmospheric science and earth observation communities, both to support their direct scientific workflow, and the curation of data products in the STFC Centre for Environmental Data Archival (CEDA). Initial JASMIN configuration and first experiences are reported here. Useful improvements in scientific workflow are presented. It is clear from the explosive growth in stored data and use that there was a pent up demand for a suitable big-data analysis environment. This demand is not yet satisfied, in part because JASMIN does not yet have enough compute, the storage is fully allocated, and not all software needs are met. Plans to address these constraints are introduced.",IEEE
10.1109/BigData.2013.6691559,Building a generic platform for big sensor data application,24,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691559,"The drive toward smart cities alongside the rising adoption of personal sensors is leading to a torrent of sensor data. While systems exist for storing and managing sensor data, the real value of such data is the insight which can be generated from it. However there is currently no platform which enables sensor data to be taken from collection, through use in models to produce useful data products. The architecture of such a platform is a current research question in the field of Big Data and Smart Cities. In this paper we explore five key challenges in this field and provide a response through a sensor data platform “Concinnity” which can take sensor data from collection to final product via a data repository and workflow system. This will enable rapid development of applications built on sensor data using data fusion and the integration and composition of models to form novel workflows. We summarize the key features of our approach, exploring how it enables value to be derived from sensor data efficiently.",IEEE
10.1109/BigData.2013.6691561,clusiVAT: A mixed visual/numerical clustering algorithm for big data,25,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691561,"Recent algorithmic and computational improvements have reduced the time it takes to build a minimal spanning tree (MST) for big data sets. In this paper we compare single linkage clustering based on MSTs built with the Filter-Kruskal method to the proposed clusiVAT algorithm, which is based on sampling the data, imaging the sample to estimate the number of clusters, followed by non-iterative extension of the labels to the rest of the big data with the nearest prototype rule. Numerical experiments with both synthetic and real data confirm the theory that clusiVAT produces true single linkage clusters in compact, separated data. We also show that single linkage fails, while clusiVAT finds high quality partitions that match ground truth labels very well. And clusiVAT is fast: it recovers the preferred c = 3 Gaussian clusters in a mixture of 1 million two-dimensional data points with 100% accuracy in 3.1 seconds.",IEEE
10.1109/BigData.2013.6691567,Algebraic dataflows for big data analysis,26,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691567,"Analyzing big data requires the support of dataflows with many activities to extract and explore relevant information from the data. Recent approaches such as Pig Latin propose a high-level language to model such dataflows. However, the dataflow execution is typically delegated to a MapRe-duce implementation such as Hadoop, which does not follow an algebraic approach, thus it cannot take advantage of the optimization opportunities of PigLatin algebra. In this paper, we propose an approach for big data analysis based on algebraic workflows, which yields optimization and parallel execution of activities and supports user steering using provenance queries. We illustrate how a big data processing dataflow can be modeled using the algebra. Through an experimental evaluation using real datasets and the execution of the dataflow with Chiron, an engine that supports our algebra, we show that our approach yields performance gains of up to 19.6% using algebraic optimizations in the dataflow and up to 39.1% of time saved on a user steering scenario.",IEEE
10.1109/BigData.2013.6691569,Robot: An efficient model for big data storage systems based on erasure coding,27,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691569,"It is well-known that with the explosive growth of data, the age of big data has arrived. How to save huge amounts of data is of great importance to both industry and academia. This paper puts forward a solution based on coding technologies in big data system that store a lot of cold data. By studying existing coding technologies and big data systems, we can not only maintain the system's reliability, but also improve the security and the utilization of storage systems. Due to the remarkable reliability and space saving rate of coding technologies, importing coding schema in to big data systems becomes prerequisite. In our presented schema, the storage node is divided into several virtual nodes to keep load balancing. By setting up different virtual node storage groups for different codec server, we can ensure system availability. And by utilizing the parallel decoding computing of the node and the block of data, we can also reduce the system recovery time when data is corrupted. Additionally, different users set different coding parameters can improve the robustness of big data storage systems. We configure various data block m and calibration block k to improve the utilization rate in the quantitative experiments. The results shows that parallel decoding speed can rise up two times than the past serial decoding speed. The encoding efficiency with ICRS coding is 34.2% higher than using CRS and 56.5% more than using RS coding equally. The decoding rate by using ICRS is 18.1% higher than using CRS and 31.1% higher than using RS averagely.",IEEE
10.1109/BigData.2013.6691570,Multilevel Active Storage for big data applications in high performance computing,28,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691570,"Given the growing importance of supporting dataintensive sciences and big data applications, an effective HPC I/O solution has become a key issue and has attracted intensive attention in recent years. Active storage has been shown effective in reducing data movement and network traffic as a potential new I/O solution. Existing prototypes and systems, however, are primarily designed for read-intensive applications. In addition, they generally assume that offloaded processing kernels have small computational demands, which makes this solution a poor fit for data-intensive operations that have significant computational demands, including write-intensive operations. In this research, we propose a new Multilevel Active Storage (MAS) solution. The new MAS design can support and handle both read- and write-intensive operations, as well as complex operations that have considerable computational demands. Experimental tests have been carried out and confirmed that the MAS approach is feasible and outperformed existing approaches. The new multilevel active storage design has a potential to deliver a high performance I/O solution for big data applications in HPC.",IEEE
10.1109/BigData.2013.6691571,GPU accelerated item-based collaborative filtering for big-data applications,29,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691571,"Recommendation systems are a popular marketing strategy for online service providers. These systems predict a customer's future preferences from the past behaviors of that customer and the other customers. Most of the popular online stores process millions of transactions per day; therefore, providing quick and quality recommendations using the large amount of data collected from past transactions can be challenging. Parallel processing power of GPUs can be used to accelerate the recommendation process. However, the amount of memory available on a GPU card is limited; thus, a number of passes may be required to completely process a large-scale dataset. This paper proposes two parallel, item-based recommendation algorithms implemented using the CUDA platform. Considering the high sparsity of the user-item data, we utilize two compression techniques to reduce the required number of passes and increase the speedup. The experimental results on synthetic and real-world datasets show that our algorithms outperform the respective CPU implementations and also the naïve GPU implementation which does not use compression.",IEEE
10.1109/BigData.2013.6691592,Using pattern-models to guide SSD deployment for Big Data applications in HPC systems,30,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691592,"Flash-memory based Solid State Drives (SSDs) embrace higher performance and lower power consumption compared to traditional storage devices (HDDs). These benefits are needed in HPC systems, especially with the growing demand of supporting Big Data applications. In this paper, we study placement and deployment strategies of SSDs in HPC systems to maximize the performance improvement, given a practical fixed hardware budget constraint. We propose a pattern-model approach to guide SSD deployment for HPC systems through two steps; characterizing workload and mapping deployment strategy. The first step is responsible for characterizing the access patterns of the workload and the second step contributes the actual deployment recommendation for Parallel File System (PFS) configuration combining with an analytical model. We have carried out initial experimental tests and the results confirmed that the proposed approach can guide placement of SSDs in HPC systems for accelerating data accesses. Our research will be helpful in guiding designs and developments for Big Data applications in current and projected HPC systems including exascale systems.",IEEE
10.1109/BigData.2013.6691612,Classification of big velocity data via cross-domain Canonical Correlation Analysis,31,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691612,"Many classification techniques work well only under a common assumption that the training and test data are drawn from the same feature space and the same distribution. However, big velocity data usually show disobedience of this assumption. For example, in the field of web-document classification, new document is continuously emerging every day. Transfer learning aims at leveraging the knowledge in labeled source domains to predict the unlabeled data in a target domain, where the distributions are different in domains. As one of the important research directions of transfer learning, one kind of approaches focus on the correspondence between pivot features and all the other specific features from different domains, to extract some relevant features that may reduce the difference between the domains, have attracted wide attention and study. However, the limitation caused by the vague meanings in different domains prevents these algorithms from further improvement. To tackle this problem, we propose a cross-domain canonical correlation analysis algorithm called CD-CCA by applying Canonical Correlation Analysis (CCA) to transfer learning. CD-CCA can learn a semantic space of multi-view correspondences from different domains respectively and transfer the knowledge by dimensionality reduction in a multi-view way. Experimental results on the 144×6 classification problems in 20Newsgroups, show that CD-CCA can significantly improve the prediction accuracy.",IEEE
10.1109/BigData.2013.6691618,Efficient large graph pattern mining for big data in the cloud,32,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691618,"Mining big graph data is an important problem in the graph mining research area. Although cloud computing is effective at solving traditional algorithm problems, mining frequent patterns of a massive graph with cloud computing still faces the three challenges: 1) the graph partition problem, 2) asymmetry of information, and 3) pattern-preservation merging. Therefore, this paper presents a new approach, the cloud-based SpiderMine (c-SpiderMine), which exploits cloud computing to process the mining of large patterns on big graph data. The proposed method addresses the above issues for implementing a big graph data mining algorithm in the cloud. We conduct the experiments with three real data sets, and the experimental results demonstrate that c-SpiderMine can significantly reduce execution time with high scalability in dealing with big data in the cloud.",IEEE
10.1109/BigData.2013.6691624,A Higher-order data flow model for heterogeneous Big Data,33,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691624,"We introduce a data flow model that supports highly parallelisable design patterns and also has useful properties for analysing data serially over extended time periods without requiring traditional Big Data computing facilities. The model ranges over a class of higher-order relations which are sufficiently expressive to represent a wide variety of unstructured, semi-structured and structured data. Using JSONMatch, our web service implementation of the model, we show that the combination of this model and higher-order representation provides a powerful and extensible framework that is particularly well suited to analysing Big Variety data in a web application context.",IEEE
10.1109/BigData.2013.6691627,Malicious URL filtering — A big data application,34,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691627,"Malicious URLs have become a channel for Internet criminal activities such as drive-by-download, spamming and phishing. Applications for the detection of malicious URLs are accurate but slow (because they need to download the content or query some Internet host information). In this paper we present a novel lightweight filter based only on the URL string itself to use before existing processing methods. We run experiments on a large dataset and demonstrate a 75% reduction in workload size while retaining at least 90% of malicious URLs. Existing methods do not scale well with the hundreds of millions of URLs encountered every day as the problem is a heavily-imbalanced, large-scale binary classification problem. Our proposed method is able to handle nearly two million URLs in less than five minutes. We generate two filtering models by using lexical features and descriptive features, and then combine the filtering results. The on-line learning algorithms are applied here not only for dealing with large-scale data sets but also for fitting the very short lifetime characteristics of malicious URLs. Our filter can significantly reduce the volume of URL queries on which further analysis needs to be performed, saving both computing time and bandwidth used for content retrieval.",IEEE
10.1109/BigData.2013.6691630,Breaking the Arc: Risk control for Big Data,35,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691630,"The use of Big Data technologies and analytics have the potential to revolutionise the world. The mass instrumentation of the planet and society is providing intelligence that is not only enhancing our personal lives, but also opening up new opportunities for addressing some of key environmental, social and economic challenges of the 21st century. Unfortunately, as with all technology, there is the potential for misuse; in the case of personal data the ability to gather, enrich and mine at extreme pace and volume could result in societal-scale privacy intrusions. We apply a model for identity across cyber and physical spaces to the question of risk control for personal-data in the context of big data analytics. Using a graphical model for identity we reflect on the response options we have and how such risk controls may or may not be effective.",IEEE
10.1109/BigData.2013.6691631,"The BTWorld use case for big data analytics: Description, MapReduce logical workflow, and empirical evaluation",36,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691631,"The commoditization of big data analytics, that is, the deployment, tuning, and future development of big data processing platforms such as MapReduce, relies on a thorough understanding of relevant use cases and workloads. In this work we propose BTWorld, a use case for time-based big data analytics that is representative for processing data collected periodically from a global-scale distributed system. BTWorld enables a data-driven approach to understanding the evolution of BitTorrent, a global file-sharing network that has over 100 million users and accounts for a third of today's upstream traffic. We describe for this use case the analyst questions and the structure of a multi-terabyte data set. We design a MapReduce-based logical workflow, which includes three levels of data dependency - inter-query, inter-job, and intra-job - and a query diversity that make the BTWorld use case challenging for today's big data processing tools; the workflow can be instantiated in various ways in the MapReduce stack. Last, we instantiate this complex workflow using Pig-Hadoop-HDFS and evaluate the use case empirically. Our MapReduce use case has challenging features: small (kilobytes) to large (250 MB) data sizes per observed item, excellent (10-6) and very poor (102) selectivity, and short (seconds) to long (hours) job duration.",IEEE
10.1109/BigData.2013.6691632,Modeling heterogeneous time series dynamics to profile big sensor data in complex physical systems,37,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691632,"While a massive amount of time series can now be collected in many physical systems, it is a challenge to build an analytic model that can correctly profile the data because those time series usually exhibit various behaviors. In this paper we propose an integrated method to address the heterogeneity issue in modeling big time series data. We first extracts relevant features to summarize the underlying dynamics of those series. We present both linear and nonlinear feature extraction techniques, as well as a procedure to determine the right extraction method for individual time series. Given extracted features, our method further models the trajectory pattern of time series in the feature space. Both a regression based and a density based method are presented to profile different types of feature trajectories. Experimental results in a real power plant illustrate that our feature extraction and trajectory model are effective to profile various time series. Our method has been used to successfully detect anomalies in the system.",IEEE
10.1109/BigData.2013.6691643,Demand response targeting using big data analytics,38,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691643,"The drive towards more sustainable power supply systems has enabled significant growth of renewable generation. This in turn has pushed the rollout of demand response (DR) programs to address a larger population of consumers. Utilities are interested in enrolling small and medium sized customers that can provide demand curtailment during periods of shortfall in renewable production. It then becomes important to be able to target the right customers among the large population, since each enrollment has a cost. The availability of high resolution information about each consumers demand consumption can significantly change how such targeting is done. This paper develops a methodology for large scale targeting that combines data analytics and a scalable selection procedure. We propose an efficient customer selection method via stochastic knapsack problem solving and a simple response modeling in one example DR program. To cope with computation issues coming from the large size of data set, we design a novel approximate algorithm.",IEEE
10.1109/BigData.2013.6691653,Big data analytics on high Velocity streams: A case study,39,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691653,"Big data management is often characterized by three Vs: Volume, Velocity and Variety. While traditional batch-oriented systems such as MapReduce are able to scale-out and process very large volumes of data in parallel, they also introduce some significant latency. In this paper, we focus on the second V (Velocity) of the Big Data triad; We present a case-study where we use a popular open-source stream processing engine (Storm) to perform real-time integration and trend detection on Twitter and Bitly streams. We describe our trend detection solution below and experimentally demonstrate that our architecture can effectively process data in real-time - even for high-velocity streams.",IEEE
10.1109/BigData.2013.6691666,Visualization and rhetoric: Key concerns for utilizing big data in humanities research: A case study of vaccination discourses: 1918-1919,40,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691666,"Visualization of data mining results is the linchpin of successful research in the humanities that uses computational techniques. This paper describes efforts to utilize “big data” in a case study of news reporting on vaccination before, during, and after the 1918 influenza pandemic, focusing primarily on the conventions underlying methods of data extraction, data visualization practices, and the rhetorical impact of visualization design choices on researchers' observations and interpretive decisions. Purposeful attention to visualization and the methodological conventions that are embedded in particular visualization practices will allow humanists to have more confidence in their interpretations of big data, a key element in the acceptance of data mining as a valuable method for humanities research.",IEEE
10.1109/BigData.2013.6691667,"Humanities ‘big data’: Myths, challenges, and lessons",41,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691667,"This paper argues that there have always been `big data' in the humanities, and challenges commonly held myths in this regard. It does so by discussing the case of transnational research on dispersed communities. Concluding, it examines the lessons humanities and sciences can learn from each other.",IEEE
10.1109/BigData.2013.6691670,Bibliographic records as humanities big data,42,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691670,"Most discussion hitherto of big data in the humanities has assumed that it is characterized by its heterogeneous nature. This paper examines the extent to which bibliographic records generated by libraries represent a more homogenous form of humanities big data, more closely related to the observational big data generated by scientific data. It is suggested from an examination of the British Library catalogue that, while superficially bibliographic records appear to be created according to consistent standards and form a more homogenous dataset, close examination reveals that bibliographical records often go through a marked process of historical development. However, the critical methods require to disaggregate such data are perhaps analogous to those used in some scientific disciplines.",IEEE
10.1109/BigData.2013.6691672,A concept of Generic Workspace for Big Data Processing in Humanities,43,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691672,"Big Data challenges often require application of new data processing paradigms (like MapReduce), and corresponding software solutions (e. g. Hadoop). This trend causes a pressure on both cyber-infrastructure providers (to quickly integrate new services) and infrastructure users (to quickly learn to use new tools). In this paper we present the concept of DARIAH Generic Workspace for Big Data Processing in eHumanities which alleviates the aforementioned problems. It establishes a common integration layer, thus enables a quick integration of new services, and by providing unified interfaces, allows the users to start using new tools without learning their internal details. We describe the overall architecture and implementation details of the working prototype. The presented concept is generic enough to be applied in other emerging cyber-infrastructures for humanities.",IEEE
10.1109/BigData.2013.6691678,A case study on entity Resolution for Distant Processing of big Humanities data,44,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691678,"At the forefront of big data in the Humanities, collections management can directly impact collections access and reuse. However, curators using traditional data management methods for tasks such as identifying redundant from relevant and related records, a small increase in data volume can significantly increase their workload. In this paper, we present preliminary work aimed at assisting curators in making important data management decisions for organizing and improving the overall quality of large unstructured Humanities data collections. Using Entity Resolution as a conceptual framework, we created a similarity model that compares directories and files based on their implicit metadata, and clusters pairs of closely related directories. Useful relationships between data are identified and presented through a graphical user interface that allows qualitative evaluation of the clusters and provides a guide to decide on data management actions. To evaluate the model's performance, we experimented with a test collection and asked the curator to classify the clusters according to four model cluster configurations that consider the presence of related and duplicate information. Evaluation results suggest that the model is useful for making data management action decisions.",IEEE
10.1109/BigData.2013.6691684,Business model canvas perspective on big data applications,45,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691684,"Large and complex data that becomes difficult to be handled by traditional data processing applications triggers the development of big data applications which have become more pervasive than ever before. In the era of big data, data exploration and analysis turned into a difficult problem in many sectors such as the smart routing and health care sectors. Companies which can adapt their businesses well to leverage big data have significant advantages over those that lag this capability. The need for exploring new approaches to address the challenges of big data forces companies to shape their business models accordingly. In this paper, we summarize and share our findings regarding the business models deployed in big data applications in different sectors. We analyze existing big data applications by taking into consideration the core elements of a business (via business model canvas) and present how these applications provide value to their customers by making profit out of using big data.",IEEE
10.1109/BigData.2013.6691687,A study of innovation network database Construction by using big data and an enterprise strategy model,46,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691687,"This paper presents a method of extracting enterprises' strategy alliance data from massive text on the Internet using the text analysis technique; And also enterprise alliance innovation networks are constructed based on the existing alliance database; Meanwhile, the process of partners choice is considered by game model and a game model of average strategy among enterprises is built.",IEEE
10.1109/BigData.2013.6691691,Understanding the value of (big) data,47,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691691,"This paper acts as a primer on an economic outlook at the value and pricing of big data. We introduce a simple taxonomy, discuss rights to access and analyze the case of big data as a common pool resource.",IEEE
10.1109/BigData.2013.6691693,Memory system characterization of big data workloads,48,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691693,"Two recent trends that have emerged include (1) Rapid growth in big data technologies with new types of computing models to handle unstructured data, such as map-reduce and noSQL (2) A growing focus on the memory subsystem for performance and power optimizations, particularly with emerging memory technologies offering different characteristics from conventional DRAM (bandwidths, read/write asymmetries). This paper examines how these trends may intersect by characterizing the memory access patterns of various Hadoop and noSQL big data workloads. Using memory DIMM traces collected using special hardware, we analyze the spatial and temporal reference patterns to bring out several insights related to memory and platform usages, such as memory footprints, read-write ratios, bandwidths, latencies, etc. We develop an analysis methodology to understand how conventional optimizations such as caching, prediction, and prefetching may apply to these workloads, and discuss the implications on software and system design.",IEEE
10.1109/BigData.2013.6691701,An ensemble MIC-based approach for performance diagnosis in big data platform,49,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691701,"The era of big data has began. Although applications based on big data bring considerable benefit to IT industries, governments and social organizations, they bring more challenges to the management of big data platforms which are the fundamental infrastructures due to the complexity, variety, velocity and volume of big data. To offer a healthy platform for big data applications, we propose a novel signature-based performance diagnosis approach employing MIC invariants between performance metrics. We formalize the performance diagnosis as a pattern recognition problem. The normal state of a big data application is used to train a set of MIC (Maximum Information Criterion) invariants. One performance problem occurred in the big data application is identified by a unique binary tuple consisted by a set violations of MIC invariants. All the signatures of performance problems form a diagnosis knowledge database. If the KPI (Key Performance Indicator) of the big data application deviates its normal region, our approach can identify the real culprits through looking for similar signatures in the signature database. To detect the deviation of the KPI, we propose a new metric named unpredictability based on ARIMA model. And considering the variety of big data applications, we build an ensemble performance diagnosis approach which means a unique ARIMA model and a unique set of MIC invariants are built for a specific kind of application. Through experiment evaluation in a controlled environment running a state of the art big data benchmark, we find our approach can pinpoint the real culprits of performance problems in an average 83% precision and 87% recall which is better than a correlation based and single model based performance diagnosis.",IEEE
10.1109/BigData.2013.6691704,On mixing high-speed updates and in-memory queries: A big-data architecture for real-time analytics,50,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6691704,"Up-to-date business intelligence has become a critical differentiator for the modern data-driven highly engaged enterprise. It requires rapid integration of new information on a continuous basis for subsequent analyses. ETL-based and traditionally batch-processing oriented methods of absorbing changes into a relational database schema take time, and are therefore incompatible with very low-latency demands of realtime analytics. Instead, in-memory clustered stores that employ tunable consistency mechanisms are becoming attractive since they dispense with the need to transform and transit data between storage layouts and tiers. When data is updated infrequently, in-memory approaches such as RDD transformations in Spark can suffice, but as updates become frequent, such in-memory approaches need to be extended to support dynamic datasets. This paper describes a few key additional requirements that result from having to support in-memory processing of data while updates proceed concurrently. The paper describes Real-time Analytics Foundation (RAF), an architecture to meet the new requirements. Performance of an early implementation of RAF is also described: for an unaudited TPC-H derived workload, RAF shows a node-to-node scaling ratio of 88% at 8 nodes, and for a query equivalent to Q6 in the TPC-H set, RAF is able to show 9x improvement over that of Hive-Hadoop. The paper also describes two RAF based solutions that are being put together by two independent software vendors in China.",IEEE
